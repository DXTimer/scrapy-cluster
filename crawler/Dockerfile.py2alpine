FROM python:2.7.12-alpine
MAINTAINER Madison Bahmer <madison.bahmer@istresearch.com>

ENV APP_HOME ${APP_HOME}
# copy crawler own requirements.txt with its dependencies
COPY requirements.txt ${APP_HOME}/

# Combine run command to create single intermeiate image layer
# This MANDATORY because developments dependencies are huge.
RUN mkdir -p ${APP_HOME} \
      && cd ${APP_HOME} \
      # Installing runtime dependencies
      && apk --no-cache add \
      curl \
      openssl \
      libffi \
      libxml2 \
      libxslt \
      # Installing buildtime dependencies. They will be removed at end of this
      # commands sequence.
      && apk --no-cache add --virtual build-dependencies \
      build-base \
      openssl-dev \
      libffi-dev \
      libxml2-dev \
      libxslt-dev \
      # Updating pip itself before installing packages from requirements.txt
      && pip install --no-cache-dir pip setuptools \
      # Installing pip packages from requirements.txt
      && pip install --no-cache-dir -r requirements.txt \
      # Removing build dependencies leaving image layer clean and neat
      && apk del build-dependencies

# move codebase over
COPY . ${APP_HOME}

WORKDIR ${APP_HOME}

# override settings via localsettings.py
COPY config/docker/settings.py ${APP_HOME}/crawling/localsettings.py

# copy testing script into container
COPY scripts/run_docker_tests.sh ${APP_HOME}/run_docker_tests.sh

# set up environment variables

# run command
CMD ["scrapy", "runspider", "crawling/spiders/link_spider.py"]
